{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from get_table_data import get_table_data\n",
    "\n",
    "df_table, df_ent_table = get_table_data()\n",
    "df_table[\"publishedat\"] = [d.to_pydatetime().date() for d in df_table[\"publishedat\"]]\n",
    "df_table[\"publishedat\"] =  [d.strftime('%d_%m_%Y') if not pd.isnull(d) else '' for d in df_table[\"publishedat\"]]\n",
    "\n",
    "df_ent_table[\"publishedat\"] = [d.to_pydatetime().date() for d in df_ent_table[\"publishedat\"]]\n",
    "df_ent_table[\"publishedat\"] =  [d.strftime('%d_%m_%Y') if not pd.isnull(d) else '' for d in df_ent_table[\"publishedat\"]]\n",
    "df_table[\"length\"] = [len(text) for text in df_table[\"text\"]]\n",
    "df_ent_table[\"length\"] = [len(text) for text in df_ent_table[\"text\"]]\n",
    "#df_ent_table.drop(['text'])\n",
    "\n",
    "#convert to csv\n",
    "df_ent_table.to_csv('df_ent_table.csv')\n",
    "df_table_short = df_table[['uniqueid', 'url', 'publishedat', 'length', 'source', 'score', 'text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import  defaultdict\n",
    "df_ents_current_dict = defaultdict(list)\n",
    "df_ents_sim_dict = defaultdict(dict)\n",
    "df_ents_sim = pd.DataFrame([])\n",
    "df_ents_data_set = set(df_table_short['publishedat'])\n",
    "df_table_short1 = df_table_short[(df_table_short.publishedat == '07_07_2017') | (df_table_short.publishedat == '05_07_2017') | (df_table_short.publishedat == '03_07_2017') | (df_table_short.publishedat == '01_07_2017') | (df_table_short.publishedat == '29_06_2017')]\n",
    "df_table_short_id = df_table_short1[(df_table_short.length < 10000) & (df_table_short.length > 2000) ]\n",
    "\n",
    "df_table_short_id.to_csv('df_table_short_id.csv')\n",
    "\n",
    "df_table_short_id_rand = df_table_short_id.sample(n=25, random_state=234)\n",
    "\n",
    "df_table_short_id_rand.to_csv('df_table_short_id_rand.csv')\n",
    "writer = pd.ExcelWriter('df_table_short_id_rand.xlsx', engine='xlsxwriter')\n",
    "df_table_short_id_rand.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save() \n",
    "\n",
    "rand_articles =   df_table_short_id_rand         \n",
    "rand_articles = df_table_short_id_rand[['url', 'publishedat']]\n",
    "\n",
    "rand_articles.to_csv('rand_articles.csv')\n",
    "writer = pd.ExcelWriter('rand_articles.xlsx', engine='xlsxwriter')\n",
    "rand_articles.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save() \n",
    "\n",
    "df_table_short_id_rand\n",
    "#rand_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aylienapiclient import textapi\n",
    "c = textapi.Client(\"f40063af\", \"ddd790cf8730e5934f0a416f4ac44b2a\")\n",
    "import json\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import nltk\n",
    "import re\n",
    "from nltk import tokenize\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sid = SIA()\n",
    "df_table_date = df_table_short_id_rand\n",
    "df_table_date['aylien_polarity'] = 0\n",
    "df_table_date['cleaned_text'] = 0\n",
    "df_table_date['nltk_polarity'] = 0\n",
    "df_table_date['resp'] = 0\n",
    "\n",
    "for index, row in df_table_date.iterrows():\n",
    "    df_table_date.loc[index, 'cleaned_text'] = re.sub('\\n','', row['text']) \n",
    "\n",
    "for index, row in df_table_date.iterrows():\n",
    "    aresp = c.Sentiment({'text': row['cleaned_text']})\n",
    "    resp_val = aresp['polarity']\n",
    "    df_table_date.loc[index, 'aylien_polarity'] = resp_val\n",
    "    lines_list = tokenizer.tokenize(row['cleaned_text'])\n",
    "    for sentence in lines_list:\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "    df_table_date.loc[index, 'nltk_polarity'] = ss['compound']\n",
    "    \n",
    "df_short =  df_table_date.loc[:,['uniqueid','url','score','aylien_polarity', 'nltk_polarity', 'score']]\n",
    "df_short.to_csv('df_short.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
